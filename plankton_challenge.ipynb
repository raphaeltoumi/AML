{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Algorithmic Machine Learning Challenge</h3>\n",
    "<h1>Plankton Image Classification</h1>\n",
    "<hr style=\"height:2px;border:none;color:#333;background-color:#333;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plankton comprises all the organisms freely drifting with ocean currents. These life forms are a critically important piece of oceanic ecosystems, accounting for more than half the primary production on earth and nearly half the total carbon fixed in the global carbon cycle. They also form the foundation of aquatic food webs, including those of large, commercially important fisheries. Loss of plankton populations could result in ecological upheaval as well as negative societal impacts, particularly in indigenous cultures and the developing world. Plankton’s global significance makes their population levels an ideal measure of the health of the world’s oceans and ecosystems.\n",
    "\n",
    "Traditional methods for measuring and monitoring plankton populations are time consuming and cannot scale to the granularity or scope necessary for large-scale studies. Improved approaches are needed. One such approach is through the use of underwater imagery sensors. \n",
    "\n",
    "In this challenge, which was prepared in cooperation with the Laboratoire d’Océanographie de Villefranche, jointly run by Sorbonne Université and CNRS, plankton images were acquired in the bay of Villefranche, weekly since 2013 and manually engineered features were computed on each imaged object. \n",
    "\n",
    "This challenge aims at developing solid approaches to plankton image classification. We will compare methods based on carefully (but manually) engineered features, with “Deep Learning” methods in which features will be learned from image data alone.\n",
    "\n",
    "The purpose of this challenge is for you to learn about the commonly used paradigms when working with computer vision problems. This means you can choose one of the following paths:\n",
    "\n",
    "- Work directly with the provided images, e.g. using a (convolutional) neural network\n",
    "- Work with the supplied features extracted from the images (*native* or *skimage* or both of them)\n",
    "- Extract your own features from the provided images using a technique of your choice\n",
    "\n",
    "You will find a detailed description about the image data and the features at the end of this text.\n",
    "In any case, the choice of the classifier that you decide to work with strongly depends on the choice of features.\n",
    "\n",
    "Please bear in mind that the purpose of this challenge is not simply to find the best-performing model that was released on e.g. Kaggle for a similar problem. You should rather make sure to understand the dificulties that come with this computer vision task. Moreover, you should be able to justify your choice of features/model and be able to explain its advantages and disadvantages for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "<hr style=\"height:1px;border:none;color:#333;background-color:#333;\" />    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond simply producing a well-performing model for making predictions, in this challenge we would like you to start developing your skills as a machine learning scientist.\n",
    "In this regard, your notebook should be structured in such a way as to explore the five following tasks that are expected to be carried out whenever undertaking such a project.\n",
    "The description below each aspect should serve as a guide for your work, but you are strongly encouraged to also explore alternative options and directions. \n",
    "Thinking outside the box will always be rewarded in these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>1. Data Exploration</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first broad component of your notebook should enable you to familiarise yourselves with the given data, an outline of which is given at the end of this challenge specification.\n",
    "\n",
    "What is new in this challenge is that you will be working with image data. Therefore, you should have a look at example images located in the *imgs.zip* file (see description below). If you decide to work with the native or the skimage features, make sure to understand them!\n",
    "\n",
    "Among others, this section should investigate:\n",
    "\n",
    "- Distribution of the different image dimensions (including the number of channels)\n",
    "- Distribution of the different labels that the images are assigned to\n",
    "\n",
    "The image labels are organized in a taxonomy. We will measure the final model performance for the classification into the *level2* categories. Make sure to understand the meaning of this label inside the taxonomy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>2. Data Pre-processing</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous step should give you a better understanding of which pre-processing is required for the data based on your approach:\n",
    "\n",
    "- If you decide to work with the provided features, some data cleaning may be required to make full use of all the data.\n",
    "- If you decide to extract your own features from the images, you should explain your approach in this section.\n",
    "- If you decide to work directly with the images themselves, preprocessing the images may improve your classification results. In particular, if you work with a neural network the following should be of interest to you:\n",
    "\n",
    "  - Due to the fully-connected layers (that usually come after the convolutional ones), the input needs to have a fixed dimension.\n",
    "  - Data augmentation (image rotation, scaling, cropping, etc. of the existing images) can be used to increase the size of the training data set. This may improve performance especially when little data is available for a particular class.\n",
    "  - Be aware of the computational cost! It might be worth rescaling the images to a smaller size!\n",
    "\n",
    "  All of the operations above are usually realized using a dataloader. This means that you do not need to create a modified version of the dataset and save it to disk. Instead, the dataloader processes the data \"on the fly\" and in-memory before passing it to the network.\n",
    "  \n",
    "    NB: Although aligning image sizes is necessary to train CNNs, this will prevent your classifier from learning about different object sizes as a feature. Additional gains may be achieved when also taking object sizes into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>3. Model Selection</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most important segment of this challenge involves the selection of a model that can successfully handle the given data and yield sensible predictions.\n",
    "Instead of focusing exclusively on your final chosen model, it is also important to share your thought process in this notebook by additionally describing alternative candidate models.\n",
    "\n",
    "The choice of your model is closely connected to the way you preprocessed the input data.\n",
    "\n",
    "Furthermore, there are other factors which may influence your decision:\n",
    "\n",
    "- What is the model's complexity?\n",
    "- Is the model interpretable?\n",
    "- Is the model capable of handling different data-types?\n",
    "- Does the model return uncertainty estimates along with predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>4. Parameter Optimisation</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Irrespective of your choice, it is highly likely that your model will have one or more (hyper-)parameters that require tuning.\n",
    "There are several techniques for carrying out such a procedure, including cross-validation, Bayesian optimisation, and several others.\n",
    "As before, an analysis into which parameter tuning technique best suits your model is expected before proceeding with the optimisation of your model.\n",
    "\n",
    "If you use a neural network, the optimization of hyperparameters (learning rate, weight decay, etc.) can be a very time-consuming process. In this case, your may decide to carry out smaller experiments and to justify your choice on these preliminary tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>5. Model Evaluation</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some form of pre-evaluation will inevitably be required in the preceding sections in order to both select an appropriate model and configure its parameters appropriately.\n",
    "In this final section, you may evaluate other aspects of the model such as:\n",
    "\n",
    "- Assessing the running time of your model;\n",
    "- Determining whether some aspects can be parallelised;\n",
    "- Training the model with smaller subsets of the data.\n",
    "- etc.\n",
    "\n",
    "For the evaluation of the classification results, you should use the F1 measure (see Submission Instructions). Here the focus should be on level2 classification. A classification evaluation for other labels is optional.\n",
    "\n",
    "Please note that you are responsible for creating a sensible train/validation/test split. There is no predefined held-out test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <b>N.B.</b> Please note that the items listed under each heading are neither exhaustive, nor are you expected to explore every given suggestion.\n",
    "    Nonetheless, these should serve as a guideline for your work in both this and upcoming challenges.\n",
    "    As always, you should use your intuition and understanding in order to decide which analysis best suits the assigned task.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h2>Submission Instructions</h2>\n",
    "    <hr style=\"height:1px;border:none;color:#333;background-color:#333;\" />    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal of this challenge is to construct a model for predicting Plankton (taxonomy level 2) classes.\n",
    "\n",
    "- Your submission will be the <b>HTML version of your notebook</b> exploring the various modelling aspects described above.\n",
    "\n",
    "- At the end of the notebook you should indicate your final evaluation score on a held-out test set. As an evaluation metric you should use the F1 score with the *average=macro* option as it is provided by the scikit-learn library. See the following link for more information:\n",
    "        \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h2>Dataset Description</h2>\n",
    "    <hr style=\"height:1px;border:none;color:#333;background-color:#333;\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Location of the Dataset on zoe\n",
    "The data for this challenge is located at: `/mnt/datasets/plankton/flowcam`\n",
    "\n",
    "#### * Hierachical Taxonomy Tree for Labels \n",
    "\n",
    "Each object is represented by a single image and is identified by a unique integer number. It has a name associated to it which is integrated in a hierarchical taxonomic tree. The identifications are gathered from different projects, classified by different people in different contexts, so they often target different taxonomic levels. For example, let us say we classify items of clothing along the following tree\n",
    "\n",
    "    top\n",
    "        shirt\n",
    "            long sleeves\n",
    "            short sleeves\n",
    "        sweater\n",
    "            hooded\n",
    "            no hood\n",
    "    bottom\n",
    "        pants\n",
    "            jeans\n",
    "            other\n",
    "        shorts\n",
    "        \n",
    "In a first project, images are classified to the finest level possible, but it may be the case that, on some pictures, it is impossible to determine whether a sweater has a hood or not, in which case it is simply classified as `sweater`. In the second project, the operator classified tops as `shirt` or `sweater` only, and bottoms to the finest level. In a third project, the operator only separated tops from bottoms. In such a context, the original names in the database cannot be used directly because, for example `sweater` will contain images that are impossible to determine as `hooded` or `no hood` *as well as* `hooded` and `no hood` images that were simply not classified further. If all three classes (`sweater`, `hooded`, and `no hood`) are included in the training set, it will likely confuse the classifier. For this reason, we define different target taxonomic levels:\n",
    "\n",
    "-   `level1` is the finest taxonomic level possible. In the example above, we would include `hooded` and `no hood` but discard all images in `sweater` to avoid confusion; and proceed in the same manner for other classes.\n",
    "\n",
    "-   `level2` is a grouping of underlying levels. In the example above, it would include `shirt` (which contains all images in `shirt`, `long sleeves`, and `short sleeves`), `sweater` (which, similarly would include this class and all its children), `pants` (including children), and `shorts`. So typically, `level2` contains more images (less discarding), sorted within fewer classes than `level1`, and may therefore be an easier classification problem.\n",
    "\n",
    "-   `level3` is an even broader grouping. Here it would be `top` vs `bottom`\n",
    "\n",
    "-   etc.\n",
    "\n",
    "In the Plankton Image dataset, the objects will be categorised based on a pre-defined 'level1' and 'level2'. You can opt to work on one of them, but we recommend you to work on `level2` because it is an easier classification problem.  \n",
    "\n",
    "#### * Data Structure\n",
    "\n",
    "    /mnt/datasets/plankton/flowcam/\n",
    "        meta.csv\n",
    "        taxo.csv\n",
    "        features_native.csv.gz\n",
    "        features_skimage.csv.gz\n",
    "        imgs.zip\n",
    "\n",
    "* `meta.csv` contains the index of images and their corresponding labels\n",
    "* `taxo.csv` defines the taxonomic tree and its potential groupings at various level. Note that, the information is also available in `meta.csv`. Therefore, the information in `taxo.csv` is probably useless, but at least it gives you a global view about taxonomy tree\n",
    "* `features_native.csv.gz` contain the morphological handcrafted features computed by ZooProcess. In fact, ZooProcess generates the region of interests (ROI) around each individual object from a original image of Plankton. In addition, it also computes a set of associated features measured on the object. These features are the ones contained in `features_native.csv.gz`\n",
    "* `features_skimage.csv.gz` contains the morphological features recomputed with skimage.measure.regionprops on the ROIs produced by ZooProcess.\n",
    "* `imgs.zip` contains a post-processed version of the original images. Images are named by `objid`.jpg\n",
    "\n",
    "#### * Attributes in meta.csv\n",
    "\n",
    "The file contains the image identifiers (objid) as well as the labels assigned to the images by human operators. Those are defined with various levels of precision:\n",
    "\n",
    "* <i>unique_name</i>: raw labels from operators\n",
    "* <i>level1</i>: cleaned, most detailed labels\n",
    "* <i>level2</i>: regrouped (coarser) labels\n",
    "* <i>lineage</i>: full taxonomic lineage of the class\n",
    "\n",
    "Some labels may be missing (coded ‘NA’) at a given level, meaning that the corresponding objects should be discarded for the classification at this level.\n",
    "\n",
    "#### * imgs.zip\n",
    "\n",
    "This zip archive contains an *imgs* folder that contains all the images in .jpg format. Do not extract this folder to disk! Instead you will be loading the images to memory. See the code below for a quick how-to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "def extract_zip_to_memory(input_zip):\n",
    "    '''\n",
    "    This function extracts the images stored inside the given zip file.\n",
    "    It stores the result in a python dictionary.\n",
    "    \n",
    "    input_zip (string): path to the zip file\n",
    "    \n",
    "    returns (dict): {filename (string): image_file (bytes)}\n",
    "    '''\n",
    "    input_zip=zipfile.ZipFile(input_zip)\n",
    "    return {name: BytesIO(input_zip.read(name)) for name in input_zip.namelist() if name.endswith('.jpg')}\n",
    "\n",
    "\n",
    "# img_files = extract_zip_to_memory(\"imgs.zip\")\n",
    "\n",
    "# Display an example image \n",
    "# Image.open(img_files['imgs/32738710.jpg'])\n",
    "\n",
    "# Load the image as a numpy array:\n",
    "# np_arr = np.array(Image.open(img_files['imgs/32738710.jpg']))\n",
    "\n",
    "# Be aware that the dictionary will occupy roughly 2GB of computer memory!\n",
    "# To free this memory again, run:\n",
    "# del img_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Attributes in features_native.csv.gz\n",
    "A brief outline of the availabel attributes in `features_native.csv.gz` which you can use is given below:\n",
    "\n",
    "* <i>objid</i>: same as in `meta.csv`\n",
    "* <i>area</i>: area of ROI\n",
    "* <i>meanimagegrey</i>:\n",
    "* <i>mean</i>: mean grey\n",
    "* <i>stddev</i>: standard deviation of greys\n",
    "* <i>min</i>: minimum grey\n",
    "* <i>perim.</i>: perimeter of ROI\n",
    "* <i>width, height</i>: dimensions of ROI\n",
    "* <i>major, minor</i>: length of major,minor axis of the best fitting ellipse\n",
    "* <i>angle</i>: \n",
    "* <i>circ.</i>: circularity or shape factor which can be computed by 4pi(area/perim.^2)\n",
    "* <i>feret</i>:  maximal feret diameter\n",
    "* <i>intden</i>: integrated density: mean*area\n",
    "* <i>median</i>: median grey\n",
    "* <i>skew, kurt</i>: skewness,kurtosis of the histogram of greys\n",
    "* <i>%area</i>: proportion of the image corresponding to the object\n",
    "* <i>area_exc</i>: area excluding holes\n",
    "* <i>fractal</i>: fractal dimension of the perimeter\n",
    "* <i>skelarea</i>: area of the one-pixel wide skeleton of the image ???\n",
    "* <i>slope</i>: slope of the cumulated histogram of greys\n",
    "* <i>histcum1, 2, 3</i>:  grey level at quantiles 0.25, 0.5, 0.75 of the histogram of greys\n",
    "* <i>nb1, 2, 3</i>: number of objects after thresholding at the grey levels above\n",
    "* <i>symetrieh, symetriev</i>: index of horizontal,vertical symmetry\n",
    "* <i>symetriehc, symetrievc</i>: same but after thresholding at level histcum1\n",
    "* <i>convperim, convarea</i>: perimeter,area of the convex hull of the object\n",
    "* <i>fcons</i>: contrast\n",
    "* <i>thickr</i>: thickness ratio: maximum thickness/mean thickness\n",
    "* <i>esd</i>:\n",
    "* <i>elongation</i>: elongation index: major/minor\n",
    "* <i>range</i>: range of greys: max-min\n",
    "* <i>meanpos</i>:  relative position of the mean grey: (max-mean)/range\n",
    "* <i>centroids</i>:\n",
    "* <i>cv</i>: coefficient of variation of greys: 100*(stddev/mean)\n",
    "* <i>sr</i>: index of variation of greys: 100*(stddev/range)\n",
    "* <i>perimareaexc</i>:\n",
    "* <i>feretareaexc</i>:\n",
    "* <i>perimferet</i>: index of the relative complexity of the perimeter: perim/feret\n",
    "* <i>perimmajor</i>: index of the relative complexity of the perimeter: perim/major\n",
    "* <i>circex</i>:\n",
    "* <i>cdexc</i>:\n",
    "* <i>kurt_mean</i>:\n",
    "* <i>skew_mean</i>:\n",
    "* <i>convperim_perim</i>:\n",
    "* <i>convarea_area</i>:\n",
    "* <i>symetrieh_area</i>:\n",
    "* <i>symetriev_area</i>:\n",
    "* <i>nb1_area</i>:\n",
    "* <i>nb2_area</i>:\n",
    "* <i>nb3_area</i>:\n",
    "* <i>nb1_range</i>:\n",
    "* <i>nb2_range</i>:\n",
    "* <i>nb3_range</i>:\n",
    "* <i>median_mean</i>:\n",
    "* <i>median_mean_range</i>:\n",
    "* <i>skeleton_area</i>:\n",
    "\n",
    "#### * Attributes in features_skimage.csv.gz\n",
    "Table of morphological features recomputed with skimage.measure.regionprops on the ROIs produced by ZooProcess. See http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.regionprops for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
